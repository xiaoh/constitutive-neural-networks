{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# system modules\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Math\n",
    "\n",
    "# scientific computing\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "from fipy import CellVariable,PeriodicGrid2D\n",
    "from fipy import DiffusionTerm, ExponentialConvectionTerm, DefaultAsymmetricSolver, ImplicitSourceTerm\n",
    "from fipy import MatplotlibStreamViewer\n",
    "from fipy.tools.numerix import array, reshape\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "from plotly import subplots\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# pytorch importing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-participant",
   "metadata": {},
   "source": [
    "### define the calculation domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source field = calculation domain\n",
    "dx = 0.01; dy = 0.01\n",
    "Nx = 100; Ny = 100\n",
    "Lx = dx*Nx; Ly = dy*Ny\n",
    "mesh2 = PeriodicGrid2D(dx=dx, dy=dy, nx=298, ny=298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=2e-1 # error tolerance in defining the influence region\n",
    "n=6\n",
    "cells=np.array([[-5,-4,-3,-2,-1,0,1,2,3,4,5]])\n",
    "cells_x=np.repeat(cells,cells.shape[1],axis=0)\n",
    "cells_y=np.repeat(cells.T,cells.shape[1],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-kitchen",
   "metadata": {},
   "source": [
    "### calculate the periodic covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparameters\n",
    "# sigma = 1; l1 = 0.1; l2 = 0.1\n",
    "\n",
    "# # source mesh points\n",
    "# src_xy = mesh.cellCenters.value\n",
    "\n",
    "# # rbf_kernel (create a specified covariance matrix)\n",
    "# '''\n",
    "#         rbf_kernel:  sigma**2 exp(-(|x_i - x_j|)**2 / (2*l**2))\n",
    "# '''\n",
    "# cov = np.zeros([Nx*Ny,Nx*Ny])\n",
    "# for j in range(Nx*Ny):\n",
    "#     for i in range(Nx*Ny):\n",
    "#         cov[i,j] = sigma**2 * np.exp(-((src_xy[0,i]-src_xy[0,j])**2 / (2*l1**2) + (src_xy[1,i]-src_xy[1,j])**2 / (2*l2**2)))\n",
    "\n",
    "# # Make sure covariance matrix is symmetric\n",
    "# a = np.allclose(cov, cov.T, rtol=1e-05, atol=1e-08)\n",
    "\n",
    "# cov = cov + 1e-10*np.eye(np.shape(cov)[0])\n",
    "# # Make sure your matrix is positve definite\n",
    "# b = np.any(np.linalg.eigvalsh(cov) < 0.)\n",
    "\n",
    "# if a==False:\n",
    "#     print('ERROR: Covariance matrix is not symmetric as required')\n",
    "# elif b==True:\n",
    "#     print('ERROR: Covariance matrix is not positive definite as required')\n",
    "#     print('eigenvalues: ', np.linalg.eigvalsh(cov))\n",
    "\n",
    "# # Decompose covariance matrix (Cholesky Decomposition)\n",
    "# L = np.linalg.cholesky(cov)\n",
    "# Lt = L.T\n",
    "# L_Lt = np.dot(L,Lt)\n",
    "\n",
    "# a = np.allclose(cov, L_Lt, rtol=1e-05, atol=1e-08)\n",
    "# if a==True:\n",
    "#     print('Successfully decomposed!')\n",
    "# else:\n",
    "#     print('ERROR: L_Lt != cov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('cov_matrix.npy',L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.load('cov_matrix_periodic.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-quantity",
   "metadata": {},
   "source": [
    "### Random source field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_field():\n",
    "    u = np.random.normal(size=(Nx*Ny,1))    # generate uncorrelated standard normal\n",
    "    samples = np.dot(L,u)\n",
    "    samples = (5/(samples.max()-samples.min()))*(samples-samples.min())+0\n",
    "    samples_final = samples.reshape(Ny,Nx)\n",
    "    samples_final[-1,:] = samples_final[0,:]\n",
    "    samples_final[:,-1] = samples_final[:,0]   # make sure the source is indeed periodic!\n",
    "    return samples_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peri_source(source):\n",
    "    a = np.concatenate((source[0:-1,:],source,source[1:,:]))\n",
    "    b = np.concatenate((a[:,0:-1],a,a[:,1:]),axis=1)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-cliff",
   "metadata": {},
   "source": [
    "### Source Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_src(mesh1,source):\n",
    "#     src = CellVariable(name='Source', mesh=mesh1)   \n",
    "# #     u = np.random.normal(size=(Nx*Ny,1))    # generate uncorrelated standard normal\n",
    "# #     samples = np.dot(L,u)\n",
    "# #     samples_final= (5/(samples.max()-samples.min()))*(samples-samples.min())+0\n",
    "#     samples_final = source.flatten()\n",
    "#     src.value = np.ravel(samples_final)     # reshape sample_final (10000,1) to 10000\n",
    "    \n",
    "#     return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_src2(mesh2,source):\n",
    "    src = CellVariable(name='Source_big', mesh=mesh2)   \n",
    "#     u = np.random.normal(size=(Nx*Ny,1))    # generate uncorrelated standard normal\n",
    "#     samples = np.dot(L,u)\n",
    "#     samples_final= (5/(samples.max()-samples.min()))*(samples-samples.min())+0\n",
    "    samples_final = (peri_source(source[::-1])[::-1]).flatten()\n",
    "    src.value = np.ravel(samples_final)     # reshape sample_final (90000,1) to 90000\n",
    "    \n",
    "    return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-month",
   "metadata": {},
   "source": [
    "### Velocity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_vel(mesh, C_u, alpha):\n",
    "    vel = mesh.cellCenters.copy()\n",
    "    vel.name = 'Velocity'\n",
    "    vel.value[0] = C_u * np.cos(alpha)\n",
    "    vel.value[1] = C_u * np.sin(alpha)\n",
    "    return vel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-anaheim",
   "metadata": {},
   "source": [
    "### Data Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_data(mesh, C_uniform, alpha, diff_coeff, diss_coeff):\n",
    "#     data_list = []\n",
    "#     array_list = []\n",
    "#     var = CellVariable(name='Variable',mesh=mesh)\n",
    "#     src = sample_src(mesh,random_source)\n",
    "#     vel = sample_vel(mesh, C_uniform, alpha)\n",
    "#     eq = - ExponentialConvectionTerm(coeff=vel) + DiffusionTerm(coeff=diff_coeff) - ImplicitSourceTerm(diss_coeff) + src\n",
    "#     eq.solve(var=var, solver=DefaultAsymmetricSolver(tolerance=1.e-12, iterations=10000))\n",
    "#     data = {'var': var, 'src': src, 'vel': vel, 'diff': diff_coeff, 'diss': diss_coeff}\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_big(mesh, C_uniform, alpha, diff_coeff, diss_coeff):\n",
    "    data_list = []\n",
    "    array_list = []\n",
    "    var = CellVariable(name='Variable',mesh=mesh)\n",
    "    src = sample_src2(mesh,random_source)\n",
    "    vel = sample_vel(mesh, C_uniform, alpha)\n",
    "    eq = - ExponentialConvectionTerm(coeff=vel) + DiffusionTerm(coeff=diff_coeff) - ImplicitSourceTerm(diss_coeff) + src\n",
    "    eq.solve(var=var, solver=DefaultAsymmetricSolver(tolerance=1.e-12, iterations=10000))\n",
    "    data = {'var': var, 'src': src, 'vel': vel, 'diff': diff_coeff, 'diss': diss_coeff}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-panel",
   "metadata": {},
   "source": [
    "### Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def periodic(array,h):  # dimension of array => [Ny,Nx]\n",
    "#     peri_num_x = 2*(int(N*h/Lx)+1)+1\n",
    "#     peri_num_y = 2*(int(N*h/Ly)+1)+1\n",
    "#     peri = np.zeros((peri_num_y*Ny,peri_num_x*Nx))\n",
    "#     for i in range(0,Ny*(peri_num_y-1)+1,Ny):\n",
    "#         for j in range(0,Nx*(peri_num_x-1)+1,Nx):\n",
    "#             peri[i:i+Ny,j:j+Nx] = array\n",
    "#     return peri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-implement",
   "metadata": {},
   "source": [
    "### visualization the field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_cell(var):\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(4, 2.4))\n",
    "    axes.set_title('{}'.format(var.name))\n",
    "    axes.set_xlabel('x')\n",
    "    axes.set_ylabel('y')\n",
    "    cmap = matplotlib.cm.viridis\n",
    "    xmin, ymin = var.mesh.extents['min']\n",
    "    xmax, ymax = var.mesh.extents['max']\n",
    "    data = reshape(array(var), var.mesh.shape[::-1])[::-1]\n",
    "    img = axes.imshow(data, extent=(xmin, xmax, ymin, ymax), cmap=cmap.reversed())\n",
    "    plt.colorbar(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-vector",
   "metadata": {},
   "source": [
    "## Training and testing data from different uniform flow cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-phone",
   "metadata": {},
   "source": [
    "### Data Generation - testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_valid=5\n",
    "\n",
    "for sample in range(num_samples_valid):\n",
    "    # PDE coefficients\n",
    "    random_source = get_source_field()\n",
    "    \n",
    "    diff_coeff = 0.05\n",
    "    diss_coeff = 12\n",
    "\n",
    "    # flow parameters\n",
    "    C_max = 2.0; C_min = 0.1 \n",
    "    C_uniform = np.random.uniform(low=C_min, high=C_max)     # Uniform Flow Strength\n",
    "    alpha = 0 \n",
    "    \n",
    "    data_3_valid = sample_data_big(mesh2, C_uniform, alpha, diff_coeff, diss_coeff)\n",
    "\n",
    "    lambda1=(C_max - (((C_max**2)+(4*diff_coeff*diss_coeff))**0.5)) / (2*diff_coeff)\n",
    "    h=np.abs(np.log(epsilon)/lambda1)   # h => maximum h\n",
    "\n",
    "    print('diff     diss     U     h')\n",
    "    print('{:.3f}   {:.3f}   {:.3f}  {:.3f}'.format(diff_coeff,diss_coeff,C_uniform,h))\n",
    "    view_cell(data_3_valid['src'])\n",
    "    view_cell(data_3_valid['var'])\n",
    "\n",
    "    c_data_valid_mesh_3 = reshape(array(data_3_valid['var']), data_3_valid['var'].mesh.shape)[::-1]\n",
    "    s_data_valid_mesh_3 = reshape(array(data_3_valid['src']), data_3_valid['src'].mesh.shape)[::-1]\n",
    "    u_data_valid_mesh_3 = reshape(array(data_3_valid['vel'])[0,:], data_3_valid['vel'].mesh.shape)[::-1]\n",
    "    v_data_valid_mesh_3 = reshape(array(data_3_valid['vel'])[1,:], data_3_valid['vel'].mesh.shape)[::-1]\n",
    "\n",
    "    dataX_valid_current=np.empty([10000, 3, cells.shape[1], cells.shape[1]])\n",
    "    dataY_valid_current=np.empty([10000, 1])\n",
    "\n",
    "    data_ind=0\n",
    "    for i in range(99,199):\n",
    "        for j in range(99,199):\n",
    "            dataX_valid_current[data_ind,0,:,:]=u_data_valid_mesh_3[(cells_y*n)+i,(cells_x*n)+j]\n",
    "            dataX_valid_current[data_ind,1,:,:]=v_data_valid_mesh_3[(cells_y*n)+i,(cells_x*n)+j]\n",
    "            dataX_valid_current[data_ind,2,:,:]=s_data_valid_mesh_3[(cells_y*n)+i,(cells_x*n)+j]\n",
    "            dataY_valid_current[data_ind,0]=c_data_valid_mesh_3[i,j]\n",
    "            \n",
    "            data_ind+=1\n",
    "\n",
    "    dataX_valid_current=torch.tensor(dataX_valid_current).to(dtype=torch.float)\n",
    "    dataY_valid_current=torch.tensor(dataY_valid_current).to(dtype=torch.float)\n",
    "\n",
    "    if sample<1:\n",
    "        dataX_valid=dataX_valid_current\n",
    "        dataY_valid=dataY_valid_current\n",
    "        \n",
    "    else:\n",
    "        dataX_valid=torch.cat((dataX_valid,dataX_valid_current))\n",
    "        dataY_valid=torch.cat((dataY_valid,dataY_valid_current))\n",
    "        \n",
    "\n",
    "tensor_valid = {'X': dataX_valid, 'Y': dataY_valid}\n",
    "torch.save(tensor_valid, 'valid_11_%d'%(n)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-orange",
   "metadata": {},
   "source": [
    "### Data-Generation - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=100\n",
    "\n",
    "for sample in range(num_samples):\n",
    "    \n",
    "    random_source = get_source_field()\n",
    "    \n",
    "    # PDE coefficients\n",
    "    diff_coeff = 0.05\n",
    "    diss_coeff = 12\n",
    "\n",
    "    # flow parameters\n",
    "    C_max = 2.0; C_min = 0.1\n",
    "    C_uniform = np.random.uniform(low=C_min, high=C_max)     # Uniform Flow Strength\n",
    "    alpha = 0 \n",
    "\n",
    "    data_3 = sample_data_big(mesh2, C_uniform, alpha, diff_coeff, diss_coeff)\n",
    "\n",
    "    lambda1=(C_max - (((C_max**2)+(4*diff_coeff*diss_coeff))**0.5)) / (2*diff_coeff)\n",
    "    h=np.abs(np.log(epsilon)/lambda1)   # h => maximum h\n",
    "\n",
    "    c_data_mesh_3 = reshape(array(data_3['var']), data_3['var'].mesh.shape)[::-1]\n",
    "    s_data_mesh_3 = reshape(array(data_3['src']), data_3['src'].mesh.shape)[::-1]\n",
    "    u_data_mesh_3 = reshape(array(data_3['vel'])[0,:], data_3['vel'].mesh.shape)[::-1]\n",
    "    v_data_mesh_3 = reshape(array(data_3['vel'])[1,:], data_3['vel'].mesh.shape)[::-1]\n",
    "\n",
    "    dataX_train_current=np.empty([10000, 3, cells.shape[1], cells.shape[1]])\n",
    "    dataY_train_current=np.empty([10000, 1])\n",
    "    \n",
    "    data_ind=0\n",
    "    for i in range(99,199):\n",
    "        for j in range(99,199):\n",
    "            dataX_train_current[data_ind,0,:,:]=u_data_mesh_3[(cells_y*n)+i,(cells_x*n)+j]\n",
    "            dataX_train_current[data_ind,1,:,:]=v_data_mesh_3[(cells_y*n)+i,(cells_x*n)+j]\n",
    "            dataX_train_current[data_ind,2,:,:]=s_data_mesh_3[(cells_y*n)+i,(cells_x*n)+j]\n",
    "            dataY_train_current[data_ind,0]=c_data_mesh_3[i,j]\n",
    "             \n",
    "            data_ind+=1\n",
    "\n",
    "    dataX_train_current=torch.tensor(dataX_train_current).to(dtype=torch.float)\n",
    "    dataY_train_current=torch.tensor(dataY_train_current).to(dtype=torch.float)\n",
    "\n",
    "    if sample<1:\n",
    "        dataX_train=dataX_train_current\n",
    "        dataY_train=dataY_train_current\n",
    "    else:\n",
    "        dataX_train=torch.cat((dataX_train,dataX_train_current))\n",
    "        dataY_train=torch.cat((dataY_train,dataY_train_current))\n",
    "\n",
    "tensor_train = {'X': dataX_train, 'Y': dataY_train}\n",
    "torch.save(tensor_train, 'train_11_%d'%(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-matrix",
   "metadata": {},
   "source": [
    "### Neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Network, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=4, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X1 = X[:,0:2,:,:]\n",
    "        X2 = X[:,2:3,:,:]\n",
    "        out = self.relu(self.conv1(X1))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.relu(self.conv3(out))\n",
    "        out = self.relu(self.conv4(out))\n",
    "        out = self.relu(self.conv5(out))\n",
    "        G = self.conv6(out)\n",
    "        dim1 = X.size()[0]\n",
    "        c = torch.sum(G*X2,axis=(1,2,3))\n",
    "        c = c.reshape(dim1,1)\n",
    "        \n",
    "        return G,c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-information",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, num_epoch):\n",
    "    train_err_hist = torch.cuda.FloatTensor(1,1).fill_(0)\n",
    "    valid_err_hist = torch.cuda.FloatTensor(1,1).fill_(0)\n",
    "    train_loss_hist = torch.cuda.FloatTensor(1,1).fill_(0)\n",
    "    valid_loss_hist = torch.cuda.FloatTensor(1,1).fill_(0)\n",
    "\n",
    "    for epoch in range(num_epoch+1):\n",
    "        train_loss_array = torch.cuda.FloatTensor(1,1).fill_(0)\n",
    "        train_err_rate_num = torch.cuda.FloatTensor(1,1).fill_(0)\n",
    "        train_err_rate_den = torch.cuda.FloatTensor(1,1).fill_(0)\n",
    "        valid_loss_array = torch.cuda.FloatTensor(1,1).fill_(0)\n",
    "        valid_err_rate_num = torch.cuda.FloatTensor(1,1).fill_(0)\n",
    "        valid_err_rate_den = torch.cuda.FloatTensor(1,1).fill_(0)\n",
    "        \n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            features, target = data\n",
    "            optimizer.zero_grad()\n",
    "            G_train, forward = model(features)\n",
    "            loss = loss_fn(forward, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_array = torch.cat((train_loss_array, torch.cuda.FloatTensor([[loss.item()]])))\n",
    "            train_err_num, train_err_den = report_err_rate(target, forward)\n",
    "            train_err_rate_num = torch.cat((train_err_rate_num, (train_err_num.view(1,-1))**2), 0)\n",
    "            train_err_rate_den = torch.cat((train_err_rate_den, (train_err_den.view(1,-1))**2), 0)\n",
    "\n",
    "        train_loss = torch.mean(train_loss_array)\n",
    "        train_err_rate = 100*((torch.sum(train_err_rate_num, 0))**0.5)/((torch.sum(train_err_rate_den, 0))**0.5)\n",
    "\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data_valid in enumerate(valid_loader):\n",
    "                features_valid, target_valid = data_valid\n",
    "                G_valid, forward_valid = model(features_valid)\n",
    "                pred_loss = loss_fn(forward_valid, target_valid)\n",
    "\n",
    "                valid_loss_array = torch.cat((valid_loss_array, torch.cuda.FloatTensor([[loss.item()]])))\n",
    "                valid_err_num, valid_err_den = report_err_rate(target_valid, forward_valid)\n",
    "                valid_err_rate_num = torch.cat((valid_err_rate_num, (valid_err_num.view(1,-1))**2), 0)\n",
    "                valid_err_rate_den = torch.cat((valid_err_rate_den, (valid_err_den.view(1,-1))**2), 0)\n",
    "\n",
    "            valid_loss = torch.mean(valid_loss_array)\n",
    "            valid_err_rate = 100*((torch.sum(valid_err_rate_num, 0))**0.5)/((torch.sum(valid_err_rate_den, 0))**0.5)\n",
    "        \n",
    "        if ((train_err_rate <= 2.5) and (valid_err_rate <= 2.5)):\n",
    "            torch.save(model, 'model_gpu.pt')\n",
    "        \n",
    "        verb = True if (epoch >= 50) and (epoch % 10 == 0) else False\n",
    "        if (verb):\n",
    "            train_loss_hist = torch.cat((train_loss_hist, torch.cuda.FloatTensor([[train_loss]])))\n",
    "            train_err_hist = torch.cat((train_err_hist, train_err_rate.view(1,-1)), 0)\n",
    "            valid_loss_hist = torch.cat((valid_loss_hist, torch.cuda.FloatTensor([[valid_loss]])))\n",
    "            valid_err_hist = torch.cat((valid_err_hist, valid_err_rate.view(1,-1)), 0)\n",
    "        verb = True if (epoch % 50 == 0) else False\n",
    "        if (verb) :\n",
    "            print('{:4}   lr: {:.2e}   train_loss: {:.2e}   valid_loss: {:.2e}   train_error:{:7.2f}%   valid_error:{:7.2f}%' \\\n",
    "                  .format(epoch, exp_lr_scheduler.get_lr()[0], train_loss, valid_loss, train_err_rate[0], valid_err_rate[0]))\n",
    "            \n",
    "    print('Finished Training')\n",
    "    return train_loss_hist, train_err_hist, valid_loss_hist, valid_err_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_err_rate(target, forward):\n",
    "    errRate_sigma_num = torch.norm(forward - target, dim = 0)\n",
    "    errRate_sigma_den = torch.norm(target, dim = 0)\n",
    "    return errRate_sigma_num, errRate_sigma_den"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-camel",
   "metadata": {},
   "source": [
    "### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_valid = torch.load('valid_11_6')\n",
    "tensor_train = torch.load('train_11_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX_valid = tensor_valid['X']\n",
    "dataY_valid = tensor_valid['Y']\n",
    "dataX_train = tensor_train['X']\n",
    "dataY_train = tensor_train['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduc_data_size = int(dataX_train.shape[0] * (50/100)) # downsampling\n",
    "ind = list(range(dataX_train.shape[0]))\n",
    "np.random.shuffle(ind)\n",
    "train_ind = ind[:reduc_data_size]\n",
    "dataX_train = dataX_train[train_ind]\n",
    "dataY_train = dataY_train[train_ind]\n",
    "print('Reduced Training Data Size: {}   {}'.format(dataX_train.shape, dataY_train.shape))\n",
    "print('Validation Data Size:       {}   {}'.format(dataX_valid.shape, dataY_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda:0')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "    \n",
    "device_cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX_train = dataX_train.to(device)\n",
    "dataY_train = dataY_train.to(device)\n",
    "dataX_valid = dataX_valid.to(device)\n",
    "dataY_valid = dataY_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating datasets\n",
    "dataset_train = TensorDataset(dataX_train,dataY_train)\n",
    "dataset_valid = TensorDataset(dataX_valid,dataY_valid)\n",
    "\n",
    "#creating batches from dataset\n",
    "batch_size_train = 1024       \n",
    "batch_size_valid = dataX_valid.shape[0]\n",
    "\n",
    "train_loader = DataLoader(dataset = dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "valid_loader = DataLoader(dataset = dataset_valid, batch_size=batch_size_valid, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "model = CNN_Network()\n",
    "model.to(device)\n",
    "loss_fn = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "para_count = count_parameters(model)\n",
    "print('Total Learnable Parameters: {}'.format(para_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "num_epoch = 2000\n",
    "learning_rate = 1e-3\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=600, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "training_loss_history, training_error_history, valid_loss_history, valid_error_history = train(train_loader, valid_loader, num_epoch)\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.1f s' % (elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_gpu_final.pt')\n",
    "model.to(device_cpu)\n",
    "torch.save(model, 'model_cpu_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-complement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
